# EvalVerse 使用指南

> 📖 [English User Guide](USAGE.md)

本指南将帮助您快速上手 EvalVerse，并学会使用所有功能。

---

## 目录

- [数据集管理](#数据集管理)
- [评估器管理](#评估器管理)
- [模型配置](#模型配置)
- [模型集管理](#模型集管理)
- [实验管理](#实验管理)

---

## 数据集管理

### 创建数据集

1. 从侧边栏菜单进入 **数据集**
2. 点击 **创建数据集** 按钮
3. 填写数据集信息：
   - **名称**：输入唯一的数据集名称
   - **描述**：（可选）添加描述信息
   - **字段**：配置数据集字段
4. 点击 **创建**

![创建数据集](./screenshots/dataset-create.png)

### 导入数据

1. 打开数据集详情页面
2. 点击 **导入** 按钮
3. 选择数据文件（CSV、JSON 或 Excel 格式）
4. 配置导入设置：
   - 字段映射
   - 导入选项
5. 点击 **开始导入** 并等待完成

![导入数据集](./screenshots/dataset-import.png)

### 管理数据集版本

1. 在数据集详情页面，进入 **版本** 标签
2. 查看所有数据集版本
3. 点击 **创建版本** 创建新版本
4. 对比不同版本

![数据集版本](./screenshots/dataset-versions.png)

### 查看和编辑数据项

1. 在数据集详情页面，进入 **数据项** 标签
2. 浏览数据集项
3. 点击数据项查看详情
4. 使用 **编辑** 修改数据项内容
5. 使用 **添加数据项** 手动添加新项

![数据集项](./screenshots/dataset-items.png)

---

## 评估器管理

### 创建代码评估器

1. 从侧边栏进入 **评估器**
2. 点击 **创建评估器** → 选择 **代码评估器**
3. 填写评估器信息：
   - **名称**：输入评估器名称
   - **描述**：添加描述
4. 在代码编辑器中编写评估代码：
   ```python
   def evaluate(input_data, actual_output):
       # 您的评估逻辑
       score = calculate_score(actual_output)
       reason = "评估结果"
       return score, reason
   ```
5. 配置输入/输出模式
6. 点击 **提交** 创建评估器

![创建代码评估器](./screenshots/evaluator-code-create.png)

### 创建prompt评估器

1. 进入 **评估器** → **创建评估器** → 选择 **prompt评估器**
2. 填写基本信息
3. 配置提示模板：
   - 定义评估提示词
   - 设置输入/输出模式
   - 配置模型设置
4. 使用调试面板测试评估器
5. 点击 **提交** 创建

![创建提示评估器](./screenshots/evaluator-prompt-create.png)

### 管理评估器版本

1. 打开评估器详情页面
2. 进入 **版本** 标签
3. 查看所有版本及其状态
4. 点击 **创建版本** 创建新版本
5. 使用 diff 视图对比版本

![评估器版本](./screenshots/evaluator-versions.png)

### 测试评估器

1. 在评估器详情页面，使用 **调试** 面板
2. 输入测试数据
3. 点击 **运行** 测试评估器
4. 查看输出和评分

![评估器调试](./screenshots/evaluator-debug.png)

---

## 模型配置

### 添加模型配置

1. 从侧边栏进入 **模型配置**
2. 点击 **添加模型配置**
3. 填写配置信息：
   - **配置名称**：此配置的唯一名称
   - **模型类型**：选择模型类型（OpenAI、Anthropic 等）
   - **模型版本**：输入模型版本
   - **API 密钥**：输入您的 API 密钥
   - **API 地址**：（可选）自定义 API 端点
   - **Temperature**：设置温度参数
   - **Max Tokens**：设置最大 token 数
   - **超时时间**：设置请求超时
4. 点击 **保存**

![模型配置](./screenshots/model-config.png)

---

## 模型集管理

### 创建模型集

1. 从侧边栏进入 **模型集**
2. 点击 **创建模型集**
3. 选择模型集类型：
   - **智能体 API**：用于基于智能体的 API
   - **LLM 模型**：用于 LLM 模型
4. 配置模型集：
   - **名称**：输入模型集名称
   - **描述**：添加描述
   - **配置**：配置模型参数
5. 点击 **保存**

![模型集](./screenshots/model-set.png)

---

## 实验管理

### 创建实验

1. 从侧边栏进入 **实验**
2. 点击 **创建实验**
3. 按照分步向导操作：

   **步骤 1：基本信息**
   - 输入实验名称和描述

   **步骤 2：配置数据集**
   - 选择数据集
   - 选择数据集版本

   **步骤 3：配置评估对象**（可选）
   - 选择评估对象类型（Prompt、API 或无）
   - 配置目标设置

   **步骤 4：配置评估器**
   - 选择一个或多个评估器
   - 选择评估器版本

   **步骤 5：高级设置**
   - 设置并发数
   - 配置超时设置
   - 设置实验类型（离线/在线）

   **步骤 6：确认配置**
   - 检查所有设置
   - 点击 **创建** 开始实验

![创建实验](./screenshots/experiment-create.png)

### 运行实验

1. 创建实验后，将自动开始运行
2. 查看实验详情页面以查看：
   - 实时进度
   - 当前状态
   - 运行统计
3. 等待实验完成

![实验运行中](./screenshots/experiment-running.png)

### 查看实验结果

1. 打开实验详情页面
2. 进入 **结果** 标签
3. 查看详细结果：
   - 单项评分
   - 评估原因
   - 实际输出
   - 错误信息（如有）
4. 使用筛选器搜索和过滤结果

![实验结果](./screenshots/experiment-results.png)

### 查看统计信息

1. 在实验详情页面，进入 **统计** 标签
2. 查看聚合统计：
   - 平均分数
   - 分数分布
   - 成功/失败率
   - 评估器性能对比

![实验统计](./screenshots/experiment-statistics.png)


---

## 技巧和最佳实践

### 数据集管理
- 使用有意义的数据集名称和描述
- 在进行重大更改之前创建版本
- 导入大文件前验证数据
- 使用模式确保数据一致性

### 评估器开发
- 在实验中使用前充分测试评估器
- 使用版本控制跟踪评估器更改
- 清晰记录评估逻辑
- 优雅处理边缘情况和错误

### 实验配置
- 从小数据集开始测试
- 根据系统资源调整并发数
- 定期监控实验进度
- 在做出决策前仔细审查结果

### 性能优化
- 使用适当的超时设置
- 实验期间监控系统资源
- 使用模型集进行批量评估
- 导出和归档旧的实验结果

---

## 故障排除

### 常见问题

**数据集导入失败**
- 检查文件格式和编码
- 验证字段映射
- 检查文件大小限制

**评估器执行错误**
- 检查评估器代码的语法错误
- 检查输入/输出模式兼容性
- 验证模型配置

**实验卡住**
- 检查 Celery worker 状态
- 验证 Redis 连接
- 查看实验日志

**性能缓慢**
- 降低并发设置
- 检查数据库性能
- 监控系统资源

---

更多信息，请参考 [README](../README.zh.md) 或在 [GitHub](https://github.com/super6865/EvalVrese/issues) 提交问题。

